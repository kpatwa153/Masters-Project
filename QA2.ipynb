{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\retrievers\\document_compressors\\chain_extract.py:13: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain.chains.llm import LLMChain\n",
      "c:\\Users\\kenil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:775: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `ResponseSchema` to V2.\n",
      "  warn(\n",
      "c:\\Users\\kenil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\_internal\\_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import torch\n",
    "import camelot.io as camelot\n",
    "import camelot.plotting as cpl\n",
    "import IPython.display as display\n",
    "\n",
    "# LangChain and Vector Store Libraries\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.retrievers import MergerRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Transformers for Models\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers import (\n",
    "    Qwen2VLForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# Qdrant Client and Models\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682d02dc0b044648979b21a402be2b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2VLForConditionalGeneration(\n",
       "  (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "    )\n",
       "    (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Qwen2VLVisionBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): VisionSdpaAttention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): VisionMlp(\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (act): QuickGELUActivation()\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merger): PatchMerger(\n",
       "      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Qwen2VLModel(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "        (self_attn): Qwen2VLSdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing Models\n",
    "\n",
    "# Text embedding model\n",
    "text_embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\"\n",
    ")\n",
    "# Image embedding model\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Qwen Model\n",
    "qwen_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\"\n",
    ")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qwen_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Main Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting Tables from the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = camelot.read_pdf(\n",
    "    \"medical_information_pdf.pdf\", pages=\"19-end\", flavor=\"lattice\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process tables\n",
    "all_tables_data = []  # List to store processed tables\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    headers = table.df.iloc[0].tolist()  # Extract headers\n",
    "    data = table.df.iloc[1:].values.tolist()  # Extract table data\n",
    "    table_data = [\n",
    "        dict(zip(headers, row)) for row in data\n",
    "    ]  # Convert to list of dicts\n",
    "    all_tables_data.append(\n",
    "        {\"table_id\": i + 1, \"data\": table_data}\n",
    "    )  # Store with table ID\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"tables.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_tables_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ All tables extracted and saved in tables.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tables.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "for indices in data:\n",
    "    for key, value in indices.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpl.prepare_plot(tables[7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting PDF content using Fitz and camelot - CODE STARTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from content_extract import pdf_content_extraction\n",
    "\n",
    "content = pdf_content_extraction(\"pdf.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Overlap to maintain semantic relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 39\n"
     ]
    }
   ],
   "source": [
    "from embeddings import split_text\n",
    "\n",
    "split_texts = split_text(content[\"text\"])\n",
    "print(\"Number of text chunks:\", len(split_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Image Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import image_generate_embeddings\n",
    "\n",
    "image_embeddings, px, size = image_generate_embeddings(\n",
    "    content[\"images\"], clip_processor, clip_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Table Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated embeddings for 3 tables!\n"
     ]
    }
   ],
   "source": [
    "from embeddings import generate_table_embeddings\n",
    "\n",
    "table_embeddings = generate_table_embeddings(content, text_embedding_model)\n",
    "\n",
    "print(\"✅ Generated embeddings for\", len(table_embeddings), \"tables!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Vector Database Using Qdrant for storing Text, Image and Table Embeddings Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COLLECTION = \"text_collection\"\n",
    "IMAGE_COLLECTION = \"image_collection\"\n",
    "TABLE_COLLECTION = \"table_collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Storing Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text embeddings stored in Qdrant!\n"
     ]
    }
   ],
   "source": [
    "from store_embeddings import store_text\n",
    "\n",
    "text_store = store_text(split_texts, text_embedding_model, TEXT_COLLECTION)\n",
    "print(\"✅ Text embeddings stored in Qdrant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Embedding Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\":memory:\")  # Create an in-memory Qdrant client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Storing Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'image_collection' created successfully!\n",
      "Stored 22 images in Qdrant.\n"
     ]
    }
   ],
   "source": [
    "from store_embeddings import store_image_embeddings\n",
    "\n",
    "IMAGE_VECTOR_SIZE = 512\n",
    "store_image_embeddings(\n",
    "    client,\n",
    "    content[\"images\"],\n",
    "    image_embeddings,\n",
    "    px,\n",
    "    size,\n",
    "    IMAGE_COLLECTION,\n",
    "    IMAGE_VECTOR_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Storing Table Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'table_collection' created successfully!\n",
      "✅ Table embeddings stored in Qdrant successfully!\n"
     ]
    }
   ],
   "source": [
    "from store_embeddings import store_table_embeddings\n",
    "\n",
    "TABLE_VECTOR_SIZE = 768  # adjust as per your model\n",
    "store_table_embeddings(\n",
    "    client, TABLE_COLLECTION, TABLE_VECTOR_SIZE, table_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving Relevant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the parts of asmanex hfa?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = text_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.7},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_retrieval(\n",
    "    query,\n",
    "    clip_processor,\n",
    "    clip_model,\n",
    "    collection_name,\n",
    "    limit=3,\n",
    "    with_payload=True,\n",
    "    score_threshold=0.7,\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieves images based on a text query using CLIP model embeddings.\n",
    "\n",
    "    This function takes a text query, generates its embedding using the CLIP model,\n",
    "    and performs a similarity search in a specified Qdrant collection. It returns\n",
    "    the top matching images based on the query embedding and displays them.\n",
    "\n",
    "    Args:\n",
    "        query (str): The text query used to search for relevant images.\n",
    "        clip_processor (CLIPProcessor): The processor used to tokenize and preprocess input text for CLIP.\n",
    "        clip_model (CLIPModel): The CLIP model used to generate text embeddings.\n",
    "        collection_name (str): The name of the Qdrant collection containing the image embeddings.\n",
    "        limit (int, optional): The maximum number of results to return (default is 3).\n",
    "        with_payload (bool, optional): Whether to include metadata (e.g., filenames) with the results (default is True).\n",
    "        score_threshold (float, optional): The minimum similarity score for returning results (default is 0.7).\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the top matching images based on the query\n",
    "    \"\"\"\n",
    "    input_text = clip_processor(text=[query], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        text_embedding = (\n",
    "            clip_model.get_text_features(**input_text).squeeze().tolist()\n",
    "        )\n",
    "    results_with_scores = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=text_embedding,\n",
    "        limit=limit,\n",
    "        with_payload=with_payload,  # Retrieve metadata (e.g., filenames)\n",
    "        score_threshold=score_threshold,\n",
    "    )\n",
    "    results = [res.payload[\"filename\"] for res in results_with_scores]\n",
    "    if results:\n",
    "        for image_path in results:\n",
    "            img = Image.open(image_path)\n",
    "            display.display(img)\n",
    "    return results\n",
    "\n",
    "\n",
    "image_results = image_retrieval(\n",
    "    query, clip_processor, clip_model, IMAGE_COLLECTION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From Table Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kenil\\AppData\\Local\\Temp\\ipykernel_17488\\536263116.py:1: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 0.3.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
      "  table_retriever = Qdrant(\n"
     ]
    }
   ],
   "source": [
    "table_retriever = Qdrant(\n",
    "    client=client,\n",
    "    collection_name=TABLE_COLLECTION,\n",
    "    embeddings=text_embedding_model,\n",
    "    content_payload_key=\"table_text\",\n",
    ").as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.7},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Ranking the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged output: The parts of your ASMANEX HFA:\n",
      "There are 2 main parts to your ASMANEX HFA inhaler: the metal canister that holds the medicine and \n",
      "the blue plastic actuator that sprays the medicine from the canister. \n",
      "\n",
      "The inhaler also has a pink cap that covers the mouthpiece of the actuator (see Figure 1). The \n",
      "cap from the mouthpiece must be removed before use. The inhaler contains “120” actuations \n",
      "(puffs).\n",
      "Figure 1 \n",
      "\n",
      "The inhaler comes with a dose counter located on the plastic actuator (see Figure 1). The General Information about the safe and effective use of ASMANEX HFA.\n",
      "Medicines are sometimes prescribed for purposes other than those listed in a Patient Information leaflet. Do not use \n",
      "ASMANEX HFA for a condition for which it was not prescribed. Do not give your ASMANEX HFA to other people, even if \n",
      "they have the same condition that you have. It may harm them.\n",
      "This Patient Information leaflet summarizes the most important information about ASMANEX HFA. If you would like more \n",
      "information, talk with your healthcare provider. You can ask your healthcare provider or pharmacist for information about \n",
      "ASMANEX HFA that was written for healthcare professionals.\n",
      "For more information about ASMANEX HFA go to www.ASMANEX.com, or to report side effects call 1-844-674-3200.: What are the ingredients in ASMANEX HFA?\n",
      "Active ingredient: mometasone furoate \n",
      "Inactive ingredients: hydrofluoroalkane (HFA-227: 1,1,1,2,3,3,3-heptafluoropropane), ethanol and oleic acid information, talk with your healthcare provider. You can ask your healthcare provider or pharmacist for information about \n",
      "ASMANEX HFA that was written for healthcare professionals.\n",
      "For more information about ASMANEX HFA go to www.ASMANEX.com, or to report side effects call 1-844-674-3200.\n",
      "What are the ingredients in ASMANEX HFA?\n",
      "Active ingredient: mometasone furoate \n",
      "Inactive ingredients: hydrofluoroalkane (HFA-227: 1,1,1,2,3,3,3-heptafluoropropane), ethanol and oleic acid How should I store ASMANEX HFA?\n",
      "Store ASMANEX HFA at room temperature between 68°F to 77°F (20°C to 25°C). \n",
      "The contents of your ASMANEX HFA are under pressure. Do not puncture. Do not use or store near heat or open \n",
      "flame. Storage above 120°F may cause the canister to burst.\n",
      "Do not throw container into fire or incinerator.\n",
      "Keep ASMANEX HFA and all medicines out of the reach of children. 4\n",
      "Instructions for Use\n",
      "ASMANEX® HFA (AZ-ma-neks) 50 mcg\n",
      "ASMANEX® HFA 100 mcg\n",
      "ASMANEX® HFA 200 mcg\n",
      "(mometasone furoate)\n",
      "Inhalation Aerosol\n",
      "Read these Instructions for Use before you start using ASMANEX HFA and each time you get a refill. \n",
      "There may be new information. This leaflet does not take the place of talking to your healthcare provider \n",
      "about your medical condition or your treatment.\n",
      "The parts of your ASMANEX HFA:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kenil\\AppData\\Local\\Temp\\ipykernel_17488\\1775841980.py:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = merger_retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "merger_retriever = MergerRetriever(\n",
    "    retrievers=[retriever, table_retriever],\n",
    "    # weights=[0.5, 0.5],  # Adjust weights to balance retrieval sources\n",
    ")\n",
    "retrieved_docs = merger_retriever.get_relevant_documents(query)\n",
    "output = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "print(\"Merged output:\", output)\n",
    "# for doc in retrieved_docs:\n",
    "#     print(doc.page_content)\n",
    "#     print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Multimodal LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "qwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VLForConditionalGeneration(\n",
       "  (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "    )\n",
       "    (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Qwen2VLVisionBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): VisionSdpaAttention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): VisionMlp(\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (act): QuickGELUActivation()\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merger): PatchMerger(\n",
       "      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Qwen2VLModel(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "        (self_attn): Qwen2VLSdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\", \"output\"],\n",
    "    template=(\n",
    "        \"Given the following medical query and its context, as well as a relevant image, craft a well written and easy to understand Answer by extracting relevant information from the context.\\n\\n\"\n",
    "        \"Query:\\n{query}\\n\\n\"\n",
    "        \"Context:\\n{output}\\n\\n\"\n",
    "        \"Refer to the image if helpful.\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_results:\n",
    "    images = [Image.open(path) for path in image_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "The parts of your ASMANEX HFA include the metal canister that holds the medicine, the blue plastic actuator that sprays the medicine from the canister, the pink cap that covers the mouthpiece of the actuator, and the dose counter located on the plastic actuator. The inhaler also contains \"120\" actuations (puffs). The inhaler comes with a dose counter and a pink cap that must be removed before use. The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs). The inhaler contains \"120\" actuations (puffs\n"
     ]
    }
   ],
   "source": [
    "def generate_response(query, output_text, image_path=None):\n",
    "    \"\"\"\n",
    "    Generate a response using the Qwen2VL model with text and image input.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query/question.\n",
    "        output_text (str): The relevant context or data retrieved (text and table).\n",
    "        image_path (str, optional): Path to the image for multimodal input.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    # Format the prompt using LangChain template\n",
    "    prompt = prompt_template.format(query=query, output=output_text)\n",
    "\n",
    "    # Load image if provided\n",
    "    if image_path:\n",
    "        image = Image.open(image_path)\n",
    "    else:\n",
    "        image = None\n",
    "\n",
    "    # Prepare the inputs for the model\n",
    "    inputs = qwen_processor(text=prompt, images=image, return_tensors=\"pt\").to(\n",
    "        DEVICE\n",
    "    )\n",
    "\n",
    "    # Generate the output from the model\n",
    "    with torch.no_grad():\n",
    "        generated_ids = qwen_model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "    # Decode the generated tokens to text\n",
    "    response = qwen_processor.decode(\n",
    "        generated_ids[0], skip_special_tokens=True\n",
    "    )\n",
    "    summary_start = response.find(\"Answer:\") + len(\"Answer:\")\n",
    "    summary = response[summary_start:].strip()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "if image_results:\n",
    "    response = generate_response(query, output, image_path=image_results)\n",
    "else:\n",
    "    response = generate_response(query, output)\n",
    "print(\"Generated Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "\n",
    "model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "qwen_model = InferenceClient(\n",
    "    model=model_id, token=huggingface_token, timeout=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Device name:\",\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Device name:\",\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
